<script setup>
import { webaudiohead, audiodemo } from "../data/MDN/webaudio";
import stQuote from "@/components/Card/st-quote.vue";
</script>

<template>
  <div class="combox">
    <div>
      <h2>web audio api</h2>
      <div>
        <p>前言：</p>
        <stQuote class="mg-b10" :ctx="webaudiohead" />
      </div>

      <div>
        <div>
          <h3>1.概述</h3>
          <div>
            web audio
            api可以让用户使用音频上下文（audiocontext）中进行音频操作，具有模块化路由的特点（？）。<br />
            音频节点通过它们的输入输出互相连接，形成一个网或者一条链。一般这个链或网起始于一个或多个音频源。
            音频源可以提供一个片段或一个片段的音频采样数据（以数组的方式），一般，一秒钟的音频数据可以被切分成几万个这样的片段。
            这些片段可以是经过数学运算得到（OscillatorNode），也可以是音频或者视频文件读取处理（比如AudioBufferSourceNode和MediaElementAudioSourceNode），
            又或者是音频流（MediaStreamAudioSourceNode）。其实，音频文件本身就是声音的采样数据，这些数据可以来自麦克风，电子乐器，然后混合成一个单一复杂的波形。
            <br />
            这些节点的输出可以连接到其他节点的输入上，然后新节点可以对接收到的采样数据再进行其他处理，再形成一个结果流。
            一个最常见的操作就是通过把输入的采样数据放大来达到扩音器的效果（缩小也可以）
            <br />
            一个简单典型的web audio流程：<br />
            1.创建音频上下文 <br />
            2.在音频上下文里创建源 audio标签 <br />
            3.创建效果节点，例如混响，双二阶滤波器等等 <br />
            4.为音频选一个目的地，比如你的系统扬声器 <br />
            5.连接源到效果器，对目的地进行效果输出 <br />
          </div>
        </div>

        <div>
          <h3>2.基本概念</h3>
          <div>
            在正式开始api了解之前，有些音频工程的概念你需要知道。
            <p class="color-red">2.1 音频数据：什么是样本</p>
            当一个音频信号被处理时，取样意味着一个连续的信号转化为离散的信号；更具体的说，一个连续的声波（例如一个正在演奏的乐队发出的声音）
            会被转化为一系列的样本点（一个时间上离散的信号），计算机只可以处理这些离散的样本块。
            <p class="color-red">2.1 音频片段：帧，样本和声道</p>
            一个音频片段（AudioBuffer）会包含几个组成参数，一个或几个声道（1代表单声道，2代表立体声等等），一个长度（代表片段中采样帧的数目）
            和一个采样率（每秒采样帧的个数）
            <br />
            每个样本点都是一个代表该音频流在特定时间特定声道的数值的单精度浮点数。一个帧或一个采样帧是由一组在
            特定时间上的所有声道的样本点组成的---即所有声道在同一时间的样本点（立体声2个，5.1有6个等等，每个帧包含的样本点数和声道数相同）
            <br />
            采样率就是一秒钟内获取帧的个数，单位是赫兹hz，采样率越高，音频效果越好。
            <br />
            只需要用帧的数目除以采样率即可得到播放时间（单位秒）。用样本点数除以声道个数即可得到帧的数目。
            <pre>
              {{ audiodemo }}
            </pre>
            上述代码，你会得到一个立体声（两个声道）的音频片段（buffer），当它在一个频率44100赫兹（这是大部分声卡处理声音的频率）
            的音频环境中播放的时候，会持续0.5s--22050帧 / 44100 hz = 0.5s
            <br>
            在数字音频里，44100hz，是一个常见的取样频率。人耳的接受频率大约在20hz到20000hz之间，根据采样定理，采样
            频率一定要大于最终生成数据最大频率的二倍。，因此一定要大于40000hz；不仅如此，在采样之后信号还必须通过低通滤波器，
            否则会发生混叠现象。一个理想的低通滤波器会留下低于20khz的信号并完美（未完待续没看懂）
          </div>
        </div>

        <div>
          <h3>3.api</h3>
          <div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
</template>

<style lang="less" scoped>
p {
  margin: 6px 0;
}
</style>
